{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b12f5ec-15eb-4021-b006-d6cf280bada4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using provided Python SDK container image: gcr.io/cloud-dataflow/v1beta3/beam_python3.10_sdk:2.54.0\n",
      "INFO:root:Python SDK container image set to \"gcr.io/cloud-dataflow/v1beta3/beam_python3.10_sdk:2.54.0\" for Docker environment\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7f8ab270b6d0> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7f8ab270beb0> ====================\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://sandeep-apache/staging/employee-dataflow-job.1719578254.291491/pipeline.pb...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://sandeep-apache/staging/employee-dataflow-job.1719578254.291491/pipeline.pb in 0 seconds.\n",
      "WARNING:apache_beam.options.pipeline_options:Unknown pipeline options received: -f,/root/.local/share/jupyter/runtime/kernel-9af025e1-985f-42ee-8fb2-3ec25be5835b.json. Ignore if flags are used for internal purposes.\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding flag -f, single dash flags are not allowed.\n",
      "WARNING:apache_beam.options.pipeline_options:Unknown pipeline options received: -f,/root/.local/share/jupyter/runtime/kernel-9af025e1-985f-42ee-8fb2-3ec25be5835b.json. Ignore if flags are used for internal purposes.\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding flag -f, single dash flags are not allowed.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n",
      " clientRequestId: '20240628123734292431-2331'\n",
      " createTime: '2024-06-28T12:37:34.906673Z'\n",
      " currentStateTime: '1970-01-01T00:00:00Z'\n",
      " id: '2024-06-28_05_37_34-15191894352054664916'\n",
      " location: 'us-east1'\n",
      " name: 'employee-dataflow-job'\n",
      " projectId: 'techlanders-internal'\n",
      " stageStates: []\n",
      " startTime: '2024-06-28T12:37:34.906673Z'\n",
      " steps: []\n",
      " tempFiles: []\n",
      " type: TypeValueValuesEnum(JOB_TYPE_BATCH, 1)>\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2024-06-28_05_37_34-15191894352054664916]\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Submitted job: 2024-06-28_05_37_34-15191894352054664916\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobs/us-east1/2024-06-28_05_37_34-15191894352054664916?project=techlanders-internal\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2024-06-28_05_37_34-15191894352054664916 is in state JOB_STATE_PENDING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:37:38.337Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-1 in us-east1-c.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:37:40.486Z: JOB_MESSAGE_BASIC: Executing operation Write to BigQuery/BigQueryBatchFileLoads/ImpulseEmptyPC/Impulse+Write to BigQuery/BigQueryBatchFileLoads/ImpulseEmptyPC/FlatMap(<lambda at core.py:3774>)+Write to BigQuery/BigQueryBatchFileLoads/ImpulseEmptyPC/Map(decode)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:37:40.507Z: JOB_MESSAGE_BASIC: Executing operation Read from GCS/Read/Impulse+Read from GCS/Read/EmitSource+ref_AppliedPTransform_Read-from-GCS-Read-SDFBoundedSourceReader-ParDo-SDFBoundedSourceDoFn-_7/PairWithRestriction+ref_AppliedPTransform_Read-from-GCS-Read-SDFBoundedSourceReader-ParDo-SDFBoundedSourceDoFn-_7/SplitWithSizing\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:37:40.526Z: JOB_MESSAGE_BASIC: Executing operation Write to BigQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Impulse+Write to BigQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/FlatMap(<lambda at core.py:3774>)+Write to BigQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Map(decode)+Write to BigQuery/BigQueryBatchFileLoads/CopyJobNamePrefix+Write to BigQuery/BigQueryBatchFileLoads/GenerateFilePrefix+Write to BigQuery/BigQueryBatchFileLoads/LoadJobNamePrefix+Write to BigQuery/BigQueryBatchFileLoads/SchemaModJobNamePrefix\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:37:40.547Z: JOB_MESSAGE_BASIC: Starting 1 workers in us-east1...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2024-06-28_05_37_34-15191894352054664916 is in state JOB_STATE_RUNNING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:41:03.290Z: JOB_MESSAGE_BASIC: All workers have finished the startup processes and began to receive work requests.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:41:05.442Z: JOB_MESSAGE_BASIC: Finished operation Write to BigQuery/BigQueryBatchFileLoads/ImpulseEmptyPC/Impulse+Write to BigQuery/BigQueryBatchFileLoads/ImpulseEmptyPC/FlatMap(<lambda at core.py:3774>)+Write to BigQuery/BigQueryBatchFileLoads/ImpulseEmptyPC/Map(decode)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:41:05.472Z: JOB_MESSAGE_BASIC: Finished operation Read from GCS/Read/Impulse+Read from GCS/Read/EmitSource+ref_AppliedPTransform_Read-from-GCS-Read-SDFBoundedSourceReader-ParDo-SDFBoundedSourceDoFn-_7/PairWithRestriction+ref_AppliedPTransform_Read-from-GCS-Read-SDFBoundedSourceReader-ParDo-SDFBoundedSourceDoFn-_7/SplitWithSizing\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:41:06.569Z: JOB_MESSAGE_BASIC: Finished operation Write to BigQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Impulse+Write to BigQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/FlatMap(<lambda at core.py:3774>)+Write to BigQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Map(decode)+Write to BigQuery/BigQueryBatchFileLoads/CopyJobNamePrefix+Write to BigQuery/BigQueryBatchFileLoads/GenerateFilePrefix+Write to BigQuery/BigQueryBatchFileLoads/LoadJobNamePrefix+Write to BigQuery/BigQueryBatchFileLoads/SchemaModJobNamePrefix\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:41:06.696Z: JOB_MESSAGE_BASIC: Executing operation Write to BigQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs)/View-python_side_input0\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:41:06.715Z: JOB_MESSAGE_BASIC: Executing operation Write to BigQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/View-python_side_input0\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:41:06.734Z: JOB_MESSAGE_BASIC: Executing operation Write to BigQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/View-python_side_input0\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:41:06.740Z: JOB_MESSAGE_BASIC: Finished operation Write to BigQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs)/View-python_side_input0\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:41:06.756Z: JOB_MESSAGE_BASIC: Executing operation Write to BigQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/View-python_side_input0\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:41:06.758Z: JOB_MESSAGE_BASIC: Finished operation Write to BigQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/View-python_side_input0\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:41:06.771Z: JOB_MESSAGE_BASIC: Executing operation Write to BigQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/ParDo(TriggerLoadJobs)/View-python_side_input0\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:41:06.777Z: JOB_MESSAGE_BASIC: Finished operation Write to BigQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/View-python_side_input0\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:41:06.792Z: JOB_MESSAGE_BASIC: Executing operation Write to BigQuery/BigQueryBatchFileLoads/ParDo(UpdateDestinationSchema)/View-python_side_input0\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:41:06.801Z: JOB_MESSAGE_BASIC: Finished operation Write to BigQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/View-python_side_input0\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:41:06.815Z: JOB_MESSAGE_BASIC: Finished operation Write to BigQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/ParDo(TriggerLoadJobs)/View-python_side_input0\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:41:06.838Z: JOB_MESSAGE_BASIC: Finished operation Write to BigQuery/BigQueryBatchFileLoads/ParDo(UpdateDestinationSchema)/View-python_side_input0\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:41:06.942Z: JOB_MESSAGE_BASIC: Executing operation Write to BigQuery/BigQueryBatchFileLoads/GroupShardedRows/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:41:07.533Z: JOB_MESSAGE_BASIC: Finished operation Write to BigQuery/BigQueryBatchFileLoads/GroupShardedRows/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:41:07.613Z: JOB_MESSAGE_BASIC: Executing operation Write to BigQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:41:07.962Z: JOB_MESSAGE_BASIC: Finished operation Write to BigQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:41:08.053Z: JOB_MESSAGE_BASIC: Executing operation ref_AppliedPTransform_Read-from-GCS-Read-SDFBoundedSourceReader-ParDo-SDFBoundedSourceDoFn-_7/ProcessElementAndRestrictionWithSizing+Parse JSON+Write to BigQuery/BigQueryBatchFileLoads/RewindowIntoGlobal+Write to BigQuery/BigQueryBatchFileLoads/AppendDestination+Write to BigQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)+Write to BigQuery/BigQueryBatchFileLoads/IdentityWorkaround+Write to BigQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write+Write to BigQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations)+Write to BigQuery/BigQueryBatchFileLoads/GroupShardedRows/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:41:10.409Z: JOB_MESSAGE_BASIC: Finished operation ref_AppliedPTransform_Read-from-GCS-Read-SDFBoundedSourceReader-ParDo-SDFBoundedSourceDoFn-_7/ProcessElementAndRestrictionWithSizing+Parse JSON+Write to BigQuery/BigQueryBatchFileLoads/RewindowIntoGlobal+Write to BigQuery/BigQueryBatchFileLoads/AppendDestination+Write to BigQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)+Write to BigQuery/BigQueryBatchFileLoads/IdentityWorkaround+Write to BigQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write+Write to BigQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations)+Write to BigQuery/BigQueryBatchFileLoads/GroupShardedRows/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:41:10.462Z: JOB_MESSAGE_BASIC: Executing operation Write to BigQuery/BigQueryBatchFileLoads/GroupShardedRows/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:41:10.521Z: JOB_MESSAGE_BASIC: Finished operation Write to BigQuery/BigQueryBatchFileLoads/GroupShardedRows/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:41:10.566Z: JOB_MESSAGE_BASIC: Executing operation Write to BigQuery/BigQueryBatchFileLoads/GroupShardedRows/Read+Write to BigQuery/BigQueryBatchFileLoads/DropShardNumber+Write to BigQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile+Write to BigQuery/BigQueryBatchFileLoads/DestinationFilesUnion/InputIdentity+Write to BigQuery/BigQueryBatchFileLoads/IdentityWorkaround+Write to BigQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:41:10.932Z: JOB_MESSAGE_BASIC: Finished operation Write to BigQuery/BigQueryBatchFileLoads/GroupShardedRows/Read+Write to BigQuery/BigQueryBatchFileLoads/DropShardNumber+Write to BigQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile+Write to BigQuery/BigQueryBatchFileLoads/DestinationFilesUnion/InputIdentity+Write to BigQuery/BigQueryBatchFileLoads/IdentityWorkaround+Write to BigQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:41:11Z: JOB_MESSAGE_BASIC: Executing operation Write to BigQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:41:11.039Z: JOB_MESSAGE_BASIC: Finished operation Write to BigQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:41:11.080Z: JOB_MESSAGE_BASIC: Executing operation Write to BigQuery/BigQueryBatchFileLoads/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:41:11.194Z: JOB_MESSAGE_BASIC: Finished operation Write to BigQuery/BigQueryBatchFileLoads/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:41:11.304Z: JOB_MESSAGE_BASIC: Executing operation Write to BigQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read+Write to BigQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)+Write to BigQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)+Write to BigQuery/BigQueryBatchFileLoads/MapTuple(<lambda at bigquery_file_loads.py:1136>)+Write to BigQuery/BigQueryBatchFileLoads/ParDo(UpdateDestinationSchema)+Write to BigQuery/BigQueryBatchFileLoads/GroupByKey/Write+Write to BigQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/ParDo(TriggerLoadJobs)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:41:23.840Z: JOB_MESSAGE_BASIC: Finished operation Write to BigQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read+Write to BigQuery/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)+Write to BigQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)+Write to BigQuery/BigQueryBatchFileLoads/MapTuple(<lambda at bigquery_file_loads.py:1136>)+Write to BigQuery/BigQueryBatchFileLoads/ParDo(UpdateDestinationSchema)+Write to BigQuery/BigQueryBatchFileLoads/GroupByKey/Write+Write to BigQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/ParDo(TriggerLoadJobs)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:41:23.959Z: JOB_MESSAGE_BASIC: Executing operation Write to BigQuery/BigQueryBatchFileLoads/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:41:23.990Z: JOB_MESSAGE_BASIC: Executing operation Write to BigQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs)/View-python_side_input1\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:41:23.998Z: JOB_MESSAGE_BASIC: Finished operation Write to BigQuery/BigQueryBatchFileLoads/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:41:24.005Z: JOB_MESSAGE_BASIC: Executing operation Write to BigQuery/BigQueryBatchFileLoads/Flatten\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:41:24.029Z: JOB_MESSAGE_BASIC: Finished operation Write to BigQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs)/View-python_side_input1\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:41:24.067Z: JOB_MESSAGE_BASIC: Finished operation Write to BigQuery/BigQueryBatchFileLoads/Flatten\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:41:24.143Z: JOB_MESSAGE_BASIC: Executing operation Write to BigQuery/BigQueryBatchFileLoads/GroupByKey/Read+Write to BigQuery/BigQueryBatchFileLoads/MapTuple(<lambda at bigquery_file_loads.py:1141>)+Write to BigQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:41:25.240Z: JOB_MESSAGE_BASIC: Finished operation Write to BigQuery/BigQueryBatchFileLoads/GroupByKey/Read+Write to BigQuery/BigQueryBatchFileLoads/MapTuple(<lambda at bigquery_file_loads.py:1141>)+Write to BigQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:41:25.339Z: JOB_MESSAGE_BASIC: Executing operation Write to BigQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue/View-python_side_input0\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:41:25.381Z: JOB_MESSAGE_BASIC: Finished operation Write to BigQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue/View-python_side_input0\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:41:25.462Z: JOB_MESSAGE_BASIC: Executing operation Write to BigQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:41:25.644Z: JOB_MESSAGE_BASIC: Finished operation Write to BigQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:41:25.730Z: JOB_MESSAGE_BASIC: Executing operation Write to BigQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue+Write to BigQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:41:26.156Z: JOB_MESSAGE_BASIC: Finished operation Write to BigQuery/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue+Write to BigQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:41:26.208Z: JOB_MESSAGE_BASIC: Executing operation Write to BigQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:41:26.247Z: JOB_MESSAGE_BASIC: Finished operation Write to BigQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:41:26.285Z: JOB_MESSAGE_BASIC: Executing operation Write to BigQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read+Write to BigQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames/Keys+Write to BigQuery/BigQueryBatchFileLoads/RemoveTempTables/Delete\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:41:26.665Z: JOB_MESSAGE_BASIC: Finished operation Write to BigQuery/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read+Write to BigQuery/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames/Keys+Write to BigQuery/BigQueryBatchFileLoads/RemoveTempTables/Delete\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:41:26.876Z: JOB_MESSAGE_BASIC: Stopping worker pool...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-06-28T12:42:04.730Z: JOB_MESSAGE_BASIC: Worker pool stopped.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2024-06-28_05_37_34-15191894352054664916 is in state JOB_STATE_DONE\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions, GoogleCloudOptions, StandardOptions\n",
    "import json\n",
    "import logging\n",
    "from apache_beam.io.gcp.bigquery import WriteToBigQuery\n",
    "\n",
    "# Define the schema for BigQuery table\n",
    "schema = 'id:INTEGER, name:STRING, age:INTEGER, gender:STRING, position:STRING, department:STRING, location:STRING, salary:FLOAT, start_date:DATE, email:STRING, phone:STRING, address:STRING, manager:STRING, team:STRING, skills:STRING, projects:STRING, status:STRING, performance_rating:FLOAT, last_promotion:DATE, comments:STRING'\n",
    "\n",
    "class ParseJson(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        try:\n",
    "            record = json.loads(element)\n",
    "            # Convert lists to comma-separated strings for BigQuery\n",
    "            record['skills'] = ','.join(record['skills'])\n",
    "            record['projects'] = ','.join(record['projects'])\n",
    "            yield record\n",
    "        except json.JSONDecodeError as e:\n",
    "            logging.error(f\"Error decoding JSON: {e} - Element: {element}\")\n",
    "\n",
    "def run():\n",
    "    # Set your Google Cloud project ID\n",
    "    project_id = 'techlanders-internal'\n",
    "    bucket = 'gs://sandeep-apache/employee.json'\n",
    "    dataset_id = 'employee'\n",
    "    table_id = 'employee_table'\n",
    "    region = 'us-east1'  # Specify your region\n",
    "\n",
    "    # Beam pipeline options\n",
    "    options = PipelineOptions()\n",
    "    google_cloud_options = options.view_as(GoogleCloudOptions)\n",
    "    google_cloud_options.project = project_id\n",
    "    google_cloud_options.job_name = 'employee-dataflow-job'\n",
    "    google_cloud_options.staging_location = 'gs://sandeep-apache/staging'\n",
    "    google_cloud_options.temp_location = 'gs://sandeep-apache/temp'\n",
    "    google_cloud_options.region = region\n",
    "    options.view_as(StandardOptions).runner = 'DataflowRunner'\n",
    "    \n",
    "    with beam.Pipeline(options=options) as p:\n",
    "        (p\n",
    "         | 'Read from GCS' >> beam.io.ReadFromText(bucket)  # Read JSON data from GCS\n",
    "         | 'Parse JSON' >> beam.ParDo(ParseJson())  # Parse JSON and format for BigQuery\n",
    "         | 'Write to BigQuery' >> WriteToBigQuery(  # Write to BigQuery table\n",
    "                table=f'{project_id}:{dataset_id}.{table_id}',\n",
    "                schema=schema,\n",
    "                write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE,\n",
    "                create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED\n",
    "            ))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6b7ee9-be92-436e-9a0a-828b71564ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "apache-beam-2.54.0",
   "name": ".m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m121"
  },
  "kernelspec": {
   "display_name": "Apache Beam 2.54.0 (Local)",
   "language": "python",
   "name": "apache-beam-2.54.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
